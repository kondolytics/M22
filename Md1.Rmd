---
title: "Mariners 2022"
author: "Nicholas Kondo"
subtitle: 
output:
  html_document:
    df_print: paged
    toc: true 
  html_notebook: default
---

```{r setup, include= FALSE}
library(knitr)

# Change the number in set seed to your own favorite number
set.seed(4)
options(width=70)
options(scipen=99)

# this sets text outputted in code chunks to small
opts_chunk$set(tidy.opts=list(width.wrap=50),tidy=TRUE, size = "vsmall")  
opts_chunk$set(message = FALSE,                                          
               warning = FALSE,
               # "caching" stores objects in code chunks and only rewrites if you change things
               cache = TRUE, 
               cache.lazy = FALSE,
               # automatically downloads dependency files
               autodep = TRUE,
               # 
               cache.comments = FALSE,
               # 
               collapse = TRUE,
               # change fig.width and fig.height to change the code height and width by default
               fig.width = 5.5,  
               fig.height = 4.5,
               fig.align='center')
```

```{r setup-2, include= FALSE}

# Always print this out before your assignment
sessionInfo()
getwd()

```

```{r setup-3, include =FALSE}
# Loading libraries and data 

library(here)
library(dplyr)
library(ggplot2)
library(forcats)
library(viridis)
library(randomForest)
library(mgcv)
library(rpart)
library(partykit)
library(pROC)
library(caret)
library(tidymodels)
library(DMwR)
library(ROSE)
library(randomForestExplainer)

train <- read.csv(here('Datasets','train.csv'))

```

# 1.  Data Exploration 

```{r, include = FALSE}
# Exploring variable names
names(train)
```

The variable we are trying to predict, home runs, occurs less than 1% of all pitches.  
```{r}
table(train$is_hr)
```

```{r, include = FALSE}
homeruns <- train %>% 
  filter(is_hr == 1)
```

### 1.1 Pitch Location
Let's look at how the location of pitches affects the probability of hitting a home run.  This heat map aligns with our baseball inference that pitches closer to the middle of the zone are more likely to be hit for a home run.  It'd be interesting to look at heat maps for different types of pitches and separately for RHP and LHP.  Plots for these splits are located in the [Appendix](#appendix).
```{r}
# Creating generalized model
      fit <- gam(is_hr ~ s(plate_side, plate_height), family = binomial, data = train)
      # Finding predicted probabilities over a 50 x 50 grid
      x <- seq(-1.5, 1.5, length.out=50)
      y <- seq(0.5, 5, length.out=50)
      data.predict <- data.frame(plate_side = c(outer(x, y * 0 + 1)),
                                 plate_height = c(outer(x * 0 + 1, y)))
      # Creating LP model
      lp <- predict(fit, data.predict)
      # Adding the probability of hitting a home run given the location 
      data.predict$Probability <- exp(lp) / (1 + exp(lp))
      # Creating the K Zone
      topKzone <- 3.5
      botKzone <- 1.6
      inKzone <- -0.95
      outKzone <- 0.95
      kZone <- data.frame(
        x=c(inKzone, inKzone, outKzone, outKzone, inKzone),
        y=c(botKzone, topKzone, topKzone, botKzone, botKzone))
      # Constructing the plot for ALL pitchers
      ggplot(kZone, aes(x, y)) +
        geom_tile(data=data.predict, 
                  aes(x= plate_side, y= plate_height, fill= Probability)) +
        scale_fill_distiller(palette = "Spectral") +
        geom_path(lwd=1.5, col="black") +
        coord_fixed() + 
        labs(title = "HR Probability Based on Pitch Location",
             subtitle = "All Pitchers",
             caption = paste("N = ", nrow(train)))
```
 
 There are clear parameters to many of our predictors.  For example, a home run can only occur between certain parameters.  There are not many lienar trends between home runs and other variables.  As seen in the plots below, pitches have a lot 
 
```{r}

ggplot(train, aes(x = vert_approach_angle, y = is_hr, alpha = 1/100)) +
         geom_point()

ggplot(train, aes(x = horz_approach_angle, y = is_hr, alpha = 1/100)) +
         geom_point()

ggplot(train, aes(x = spin_rate, y = is_hr, alpha = 1/100)) +
         geom_point()

ggplot(train, aes(x = horz_break, y = is_hr, alpha = 1/100)) +
         geom_point()

ggplot(train, aes(x = induced_vert_break, y = is_hr, alpha = 1/100)) +
         geom_point()

```
 
 
 
 
 
### 1.2 Missing Values
Exploring the missing values across the data set.  
```{r}
sort(colSums(is.na(train)))
```

There are a lot of NA values for pitch type... let's check out the different types of pitches.  
```{r}
train$pitch_type <- as.factor(train$pitch_type)
table(train$pitch_type)
```

There are six types of pitches listed in this data set with a majority of pitches being  a fastball, and a low number of knuckleballs.  Here we are going to look at a dataset of the pitches where `pitch_type = NA`
```{r}
NA_pitchType <- train %>% 
  filter(is.na(train$pitch_type))
```
 
 
 
# 2. Feature Engineering

## 2.1 Handing Missing Values 

### Pitch Type 
After scrolling through the data set containing NA values, which can be looked at here **hyperlink**, the pitches seem to be somewhat normal- the pitches weren't spike or thrown over the backstop, and some actually were even hit for home run.  We should try to keep this data.  My next thought is that only so many pitches can fall under the six classes of pitches in the data set.  

I would assume two-seamers and cut-fastballs could fall under fastballs, but what about a pitch like a knuckle curve?  Would that be considered a curve or a knuckball?  Perhaps there are pitches that were not able to be classified.  This data should be preserved so for all pitches listed NA, it will be classified as `other`

**Provide link to dataset**

```{r}
# Changing pitch type to character, replacing NA values with "other", then converting back to factor
train1 <- train  %>% 
  select(-date, -umpire_id, -catcher_id,  -inning, -top_bottom, -y55, -pitch_id, -pitcher_id, -batter_id, -tilt)

train1$pitch_type <- as.character(train1$pitch_type)
train1$pitch_type[is.na(train1$pitch_type)] <- "Other"
train1$pitch_type <- as.factor(train1$pitch_type)

# Confirming NA values
sort(colSums(is.na(train1)))
levels(train1$pitch_type)

```

### Spin Rate
To accomodate for spin rate missing values, we are going to impute the median spin rate based on the type of pitch.  To do this first we need to separate the pitches by pitch type, compute the median, then impute the median for the missing values.  
```{r}
# Creating separate data sets for each pitch type 
fastballs <- train1 %>% 
  filter(pitch_type == 'FA')
curveballs <- train1 %>% 
  filter(pitch_type == 'CU')
changeups <- train1 %>% 
  filter(pitch_type == 'CH')
sinkers <- train1 %>% 
  filter(pitch_type == 'SI')
sliders <- train1 %>% 
  filter(pitch_type == 'SL')
knuckleballs <- train1 %>% 
  filter(pitch_type == 'KN')
others <- train1 %>% 
  filter(pitch_type == 'Other')

# Filling all NA spin_rate values with the median spin rate of that respective pitch
fastballs$spin_rate[is.na(fastballs$spin_rate)] <- median(fastballs$spin_rate, na.rm=TRUE)
curveballs$spin_rate[is.na(curveballs$spin_rate)] <- median(curveballs$spin_rate, na.rm=TRUE)
changeups$spin_rate[is.na(changeups$spin_rate)] <- median(changeups$spin_rate, na.rm=TRUE)
sinkers$spin_rate[is.na(sinkers$spin_rate)] <- median(sinkers$spin_rate, na.rm=TRUE)
sliders$spin_rate[is.na(sliders$spin_rate)] <- median(sliders$spin_rate, na.rm=TRUE)
knuckleballs$spin_rate[is.na(knuckleballs$spin_rate)] <- median(knuckleballs$spin_rate, na.rm=TRUE)
others$spin_rate[is.na(others$spin_rate)] <- median(others$spin_rate, na.rm=TRUE)

# Combining our pitch types back together
train1 <- bind_rows(fastballs, curveballs, changeups, sinkers, sliders, knuckleballs, others)

rm(fastballs, curveballs, changeups, sinkers, sliders, knuckleballs, others) # Freeing memory

sort(colSums(is.na(train1)))
```

### Other Missing/Usual Values
At this stage, we don't have many more missing values.  There are a series of unusual values where `vert_break`, `induced_break`, and `horz_break` have values of 0.0000.  We will remove the remaining unsual and missing values. 

```{r}
train1 %>% 
  filter(is.na(vert_break))
train1 %>% 
  filter(vert_break == 0)
# I find it very wierd that we are seeing values of 0.000 and I cannot explain the NA values for break, it may be random
# Break is a very important variable so to access this accurately we should just throw out these missing values 
# There is about 171 rows with NA and 450 rows of 0's across the board
# I am ok with throwing away the data 
# PCA?

# Dropping NA values
train1 <- na.omit(train1)

# dropping rows where break was not recorded or recorded as 0.0000
train1 <-subset(train1, vert_break!= 0)

train1 <- as.data.frame(unclass(train1),                     
                        stringsAsFactors = TRUE)
```

## Count Variable

Next let's add a count variable.  The reason for this is because each count is unique, and an 0-0 count should be treated differently than an 0-2 count. 
```{r}
# Creating a count variable 
train1 <- train1 %>% 
  mutate(count = paste(balls, "-", strikes)) 
train1$count <- as.factor(train1$count)

table(train1$count)
```

## Hitter-Pitcher Matchup

Is this a Righty-Right matchup?  Righty-Lefty matchup?  
```{r}
# Creating a matchup feature
      # Matchup will indicate the handedness of the matchups
      train1 <- na.omit(train1)
      train1 <- train1 %>% 
              mutate(matchup = 
                       ifelse(pitcher_side == "Right" & batter_side == "Right", "Righty-Righty",
                       ifelse(pitcher_side == "Right" & batter_side == "Left", "Righty-Lefty",
                       ifelse(pitcher_side == "Right" & batter_side == "Undefine", "Righty-Lefty",
                       ifelse(pitcher_side == "Left" & batter_side == "Left", "Lefty-Lefty",
                       ifelse(pitcher_side == "Left" & batter_side == "Right", "Lefty-Righty",
                       ifelse(pitcher_side == "Left" & batter_side == "Switch", "Lefty-Righty","NA")))))))
      
      train1$matchup <- as.factor(train1$matchup)
      
      table(train1$matchup)
```

## 2.2 Resampling for the Imbalanced Classes 

Since a home run occurs less than 1% of all pitches in the data set, a model like a random forest can predict ‘No HR’ every observation and the model would be correct 99% of the time, but that’s not useful for us.  Mulitple techniques for resolving the class imbalances were considered such as down-sampling, up-sampling, and hybrid method algorithms like SMOTE and ROSE.  This is different from splitting the data set.  The data was split into training and testing.  Once a balanced data set was created from the unbalanced training set, a prediction model was built off this augmented data set, and applied to the unbalanced test data.  

### Down-sampling
The first resampling technique was down-sampling.  Down sampling is where we randomly subset all the classes in the training set so that their class frequencies match the minority class.  This would result in using only 2% of our training data being used to fit the mode.  
```{r}
# Let's create different versions of the training set prior to model tuning

# Down sampling the data set 

    # Changing is_hr, balls, strikes, and outs to a factor
    train2 <- train1 %>%
      mutate(is_hr = ifelse(is_hr == 1, "Class2", "Class1")) %>% 
      mutate_if(is.character, factor) 

    train2 <- train2 %>% 
      rename(Class = is_hr)

pitches_split <- initial_split(train2, strata = Class)
pitches_train <- training(pitches_split)
pitches_test <- testing(pitches_split)

# Clearing up memory 
rm(train, train1)
```


```{r}
# Down sampling the data set 
set.seed(400)
down_train <- downSample(x = pitches_train[, -ncol(pitches_train)],
                         y = pitches_train$Class)

table(down_train$Class)
```

### Up-sampling
The next resampling method is up-sampling.  Up-sampling randomly samples the rare-occurring class to be the same size as the majority class.  In other words, we will now have the same amount of home runs as non-home runs.  
```{r}
# Up sampling the data
set.seed(400)
up_train <- upSample(x = pitches_train[, -ncol(pitches_train)],
                     y = pitches_train$Class)                         
table(up_train$Class) 
```

### SMOTE
SMOTE stands for *synthetic minority over-sampling technique.  This technique synthesizes new occurrences of home runs from the existing examples. 
```{r}
# SMOTE
set.seed(9560)
smote_train <- SMOTE(Class ~ ., data  = pitches_train)                         
table(smote_train$Class) 
```

### ROSE
ROSE stands for random over-sampling examples and is a bootstrap-based technique to help deal with the imbalanced classes.  
```{r}
# ROSE
set.seed(9560)
rose_train <- ROSE(Class ~ ., data  = pitches_train)$data                         
table(rose_train$Class) 
```


### Cross Validation
To evaluate the techniques  we will used a bagged classification and estimate the AUC (Area under ROC Curve) using five repeats of 10 fold cross validation.  
```{r}
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

set.seed(5627)
orig_fit <- train(Class ~ ., data = pitches_train, 
                  method = "treebag",
                  nbagg = 10,
                  metric = "ROC",
                  trControl = ctrl)
```
```{r}
set.seed(562)
down_outside <- train(Class ~ ., data = down_train,
                      method = "treebag",
                      nbagg = 10,
                      metric = "ROC",
                      trControl = ctrl)
```
```{r}
set.seed(562)
up_outside <- train(Class ~ ., data = up_train, 
                    method = "treebag",
                    nbagg = 10,
                    metric = "ROC",
                    trControl = ctrl)


```
```{r}
set.seed(562)
smote_outside <- train(Class ~ ., data = smote_train, 
                      method = "treebag",
                      nbagg = 10,
                      metric = "ROC",
                      trControl = ctrl)

```
```{r}
set.seed(562)
rose_outside <- train(Class ~ ., data = rose_train, 
                      method = "treebag",
                      nbagg = 10,
                      metric = "ROC",
                      trControl = ctrl)
```

# Modeling

## Random Forest
```{r}
rf_fit <- randomForest(Class ~ spin_rate + 
                         induced_vert_break +  horz_break + plate_height + plate_side + zone_speed + 
                         vert_approach_angle + horz_approach_angle + pitch_type + count + matchup,
                       data = smote_train,
                       type = classification,
                       mtry = 5,
                       na.action = na.roughfix,
                       ntree = 500,
                       localImp =TRUE) 

print(rf_fit)
plot(rf_fit)
```
## Model Tuning
Tuning Random Forests to determine optimal parameters (mtry)

```{r}
rf_mods <- list()
oob_err <- NULL
test_err <- NULL
for(mtry in 1:9){
  rf_fit <- randomForest(Class ~ 
                           spin_rate + 
                           induced_vert_break +  horz_break + plate_height + plate_side + zone_speed + 
                           vert_approach_angle + horz_approach_angle + pitch_type,
                         data = smote_train,
                         mtry = mtry,
                         na.action = na.roughfix,
                         ntree = 500)
  oob_err[mtry] <- rf_fit$err.rate[500]
  
  cat(mtry," ")
}

results_DF <- data.frame(mtry = 1:9, oob_err)
ggplot(results_DF, aes(x = mtry, y = oob_err)) + geom_point() + theme_minimal()
```

```{r}
rf_fit <- randomForest(Class ~ spin_rate + 
                         induced_vert_break +  horz_break + plate_height + plate_side + zone_speed + 
                         vert_approach_angle + horz_approach_angle + pitch_type + count + matchup,
                       data = smote_train,
                       type = classification,
                       mtry = 3,
                       na.action = na.roughfix,
                       ntree = 500,
                       localImp =TRUE) 

print(rf_fit)
plot(rf_fit)
```


We can see here that our top 5 most important variables were pitch type, vertical approach angle, spin rate, induced vertical break, and zone speed.   
```{r}
varImpPlot(rf_fit)
```
This is a distribution the number of trees each variable had and we can see how each variable plays a role in the random forest. We are obtaining a plot for the top ten variables according to mean minimal death which is calculated using top trees.
```{r}
library(randomForestExplainer)
plot_min_depth_distribution(rf_fit)
```





